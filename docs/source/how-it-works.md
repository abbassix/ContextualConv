# How It Works

## ðŸ§  ContextualConv Layers

`ContextualConv1d` and `ContextualConv2d` extend standard convolutions by allowing the output to be **modulated by a global context vector**.

## ðŸ”„ Forward Pass

1. Input tensor `x` is passed through a standard `nn.Conv1d` or `nn.Conv2d` layer.
2. If a context vector `c` is provided:
   - It is passed through a shared `ContextProcessor` module:
     - If `h_dim` is not set: just a `Linear(context_dim â†’ out_channels)`
     - If `h_dim` is set: an MLP `Linear â†’ ReLU â†’ Linear`
   - The result is a bias of shape `(B, out_channels)`
   - This bias is broadcast over the output and **added as a per-channel bias**

## ðŸ§© Architecture (1D/2D)

```python
x â†’ Conv1d/2d â†’ y
c â†’ ContextProcessor â†’ bias
y + bias â†’ final output
```

## âœ… Notes

- The context vector is **global** â€” it does not vary across spatial/temporal locations.
- The `ContextProcessor` is **shared** across all locations and all groups (if used).
- If `context_dim` is not set, the layer behaves like a standard convolution.
